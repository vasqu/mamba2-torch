{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T16:30:52.532858Z",
     "start_time": "2024-06-12T16:30:51.078026Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from src.ops.ssd_combined import mamba_chunk_scan_combined\n",
    "\n",
    "\n",
    "hidden_size, intermediate_size, ssm_state_size, chunk_size, num_heads, head_dim = 768, 1536, 128, 256, 1536 // 64, 64\n",
    "conv1d_dim = intermediate_size + 2 * ssm_state_size\n",
    "\n",
    "in_proj = nn.Linear(\n",
    "    in_features=hidden_size,\n",
    "    out_features=2 * (intermediate_size + ssm_state_size) + num_heads,\n",
    ")\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=conv1d_dim,\n",
    "    out_channels=conv1d_dim,\n",
    "    bias=True,\n",
    "    kernel_size=4,\n",
    "    groups=conv1d_dim,\n",
    "    padding=4 - 1,\n",
    ")\n",
    "\n",
    "dt_bias = nn.Parameter(torch.rand(size=(num_heads,)))\n",
    "dt = torch.exp(\n",
    "    torch.rand(num_heads)\n",
    "    * (math.log(0.1) - math.log(0.001))\n",
    "    + math.log(0.001)\n",
    ").clamp(min=1e-4)\n",
    "# Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "with torch.no_grad():\n",
    "    dt_bias.copy_(inv_dt)\n",
    "\n",
    "A = torch.empty(num_heads, dtype=torch.float32).uniform_(*(1, 16))\n",
    "A_log = nn.Parameter(torch.log(A))\n",
    "\n",
    "D = nn.Parameter(torch.ones(num_heads))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T16:40:37.730332Z",
     "start_time": "2024-06-12T16:40:37.653275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_states = torch.randn(size=(2, 3, 768), dtype=torch.float32)\n",
    "bsz, seq_len, _ = hidden_states.shape\n",
    "\n",
    "\n",
    "zxbcdt = in_proj(hidden_states)\n",
    "d_mlp = (zxbcdt.shape[-1] - 2 * intermediate_size - 2 * ssm_state_size - num_heads) // 2\n",
    "z0, x0, z, xBC, dt = torch.split(\n",
    "    zxbcdt,\n",
    "    [d_mlp, d_mlp, intermediate_size, intermediate_size + 2 * ssm_state_size, num_heads],\n",
    "    dim=-1\n",
    ")\n",
    "dt = F.softplus(dt + dt_bias).clamp(0, torch.inf)\n",
    "\n",
    "xBC = F.silu(\n",
    "    conv1d(xBC.transpose(1, 2))[..., :seq_len].transpose(1, 2)\n",
    ")\n",
    "x, B, C = torch.split(\n",
    "    xBC, [intermediate_size, ssm_state_size, ssm_state_size], dim=-1\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "y, last_state = mamba_chunk_scan_combined(\n",
    "    x=rearrange(x, \"b l (h p) -> b l h p\", p=head_dim).to(device),\n",
    "    dt=dt.to(device),\n",
    "    A=A.to(device),\n",
    "    B=rearrange(B, \"b l n -> b l 1 n\").to(device),\n",
    "    C=rearrange(C, \"b l n -> b l 1 n\").to(device),\n",
    "    chunk_size=chunk_size,\n",
    "    D=D.to(device),\n",
    "    z=None,\n",
    "    initial_states=None,\n",
    "    dt_bias=dt_bias.to(device),\n",
    "    dt_softplus=True,\n",
    "    seq_idx=None,\n",
    "    dt_min=0.0,\n",
    "    dt_max=float(\"inf\"),\n",
    "    return_final_states=True\n",
    ")\n",
    "y = rearrange(y, \"b l h p -> b l (h p)\")\n",
    "y.shape, z.shape, last_state.shape"
   ],
   "id": "62464fc00e61dba5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 1536]),\n",
       " torch.Size([2, 3, 1536]),\n",
       " torch.Size([2, 24, 64, 128]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
