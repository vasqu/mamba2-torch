{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T11:00:53.800856Z",
     "start_time": "2024-06-12T11:00:52.761436Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "hidden_size, intermediate_size, ssm_state_size, num_heads, head_dim = 768, 1536, 128, 1536 // 64, 64\n",
    "conv1d_dim = intermediate_size + 2 * ssm_state_size\n",
    "\n",
    "in_proj = nn.Linear(\n",
    "    in_features=hidden_size,\n",
    "    out_features=2 * (intermediate_size + ssm_state_size) + num_heads,\n",
    ")\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=conv1d_dim,\n",
    "    out_channels=conv1d_dim,\n",
    "    bias=True,\n",
    "    kernel_size=4,\n",
    "    groups=conv1d_dim,\n",
    "    padding=4 - 1,\n",
    ")\n",
    "\n",
    "dt_bias = nn.Parameter(torch.rand(size=(num_heads,)))\n",
    "dt = torch.exp(\n",
    "    torch.rand(num_heads)\n",
    "    * (math.log(0.1) - math.log(0.001))\n",
    "    + math.log(0.001)\n",
    ").clamp(min=1e-4)\n",
    "# Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "with torch.no_grad():\n",
    "    dt_bias.copy_(inv_dt)\n",
    "\n",
    "A = torch.empty(num_heads, dtype=torch.float32).uniform_(*(1, 16))\n",
    "A_log = nn.Parameter(torch.log(A))\n",
    "\n",
    "D = nn.Parameter(torch.ones(intermediate_size))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T11:00:54.347592Z",
     "start_time": "2024-06-12T11:00:53.801789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_states = torch.randn(size=(2, 3, 768), dtype=torch.float32)\n",
    "bsz, seq_len, _ = hidden_states.shape\n",
    "\n",
    "\n",
    "zxbcdt = in_proj(hidden_states)\n",
    "d_mlp = (zxbcdt.shape[-1] - 2 * intermediate_size - 2 * ssm_state_size - num_heads) // 2\n",
    "z0, x0, z, xBC, dt = torch.split(\n",
    "    zxbcdt,\n",
    "    [d_mlp, d_mlp, intermediate_size, intermediate_size + 2 * ssm_state_size, num_heads],\n",
    "    dim=-1\n",
    ")\n",
    "dt = F.softplus(dt + dt_bias).clamp(0, torch.inf)\n",
    "\n",
    "xBC = F.silu(\n",
    "    conv1d(xBC.transpose(1, 2))[..., :seq_len].transpose(1, 2)\n",
    ")\n",
    "x, B, C = torch.split(\n",
    "    xBC, [intermediate_size, ssm_state_size, ssm_state_size], dim=-1\n",
    ")\n",
    "\n",
    "A = -torch.exp(A_log)\n",
    "x = rearrange(x, \"b l (h p) -> b l h p\", h=num_heads)\n",
    "B = rearrange(B, \"b l (g n) -> b l g n\", g=1)\n",
    "C = rearrange(C, \"b l (g n) -> b l g n\", g=1)\n",
    "z = rearrange(z, \"b l (h p) -> b l h p\", h=num_heads)\n",
    "\n",
    "A = A * dt\n",
    "x = x * dt.unsqueeze(-1)\n",
    "# todo: check what D uses as skip\n",
    "D = D[None, None, :] * rearrange(x, \"b l h p -> b l (h p)\")\n",
    "print(x.shape,dt.shape)\n",
    "print(D.shape)"
   ],
   "id": "62464fc00e61dba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 24, 64]) torch.Size([2, 3, 24])\n",
      "torch.Size([2, 3, 1536])\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
