{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-12T23:16:02.703279Z",
     "start_time": "2024-06-12T23:16:01.240486Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from src.ops.ssd_combined import mamba_chunk_scan_combined\n",
    "\n",
    "\n",
    "hidden_size, intermediate_size, ssm_state_size, chunk_size, num_heads, head_dim = 768, 1536, 128, 256, 1536 // 64, 64\n",
    "conv1d_dim = intermediate_size + 2 * ssm_state_size\n",
    "\n",
    "in_proj = nn.Linear(\n",
    "    in_features=hidden_size,\n",
    "    out_features=2 * (intermediate_size + ssm_state_size) + num_heads,\n",
    ")\n",
    "out_proj = nn.Linear(\n",
    "    intermediate_size, hidden_size, \n",
    "    bias=True\n",
    ")\n",
    "conv1d = nn.Conv1d(\n",
    "    in_channels=conv1d_dim,\n",
    "    out_channels=conv1d_dim,\n",
    "    bias=True,\n",
    "    kernel_size=4,\n",
    "    groups=conv1d_dim,\n",
    "    padding=4 - 1,\n",
    ")\n",
    "\n",
    "dt_bias = nn.Parameter(torch.rand(size=(num_heads,)))\n",
    "dt = torch.exp(\n",
    "    torch.rand(num_heads)\n",
    "    * (math.log(0.1) - math.log(0.001))\n",
    "    + math.log(0.001)\n",
    ").clamp(min=1e-4)\n",
    "# Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "with torch.no_grad():\n",
    "    dt_bias.copy_(inv_dt)\n",
    "\n",
    "A = torch.empty(num_heads, dtype=torch.float32).uniform_(*(1, 16))\n",
    "A_log = nn.Parameter(torch.log(A))\n",
    "\n",
    "D = nn.Parameter(torch.ones(num_heads))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T23:16:02.711083Z",
     "start_time": "2024-06-12T23:16:02.704456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def forward(hidden_states, initial_state=None, return_final_state=False, cache=None, use_cache=False):\n",
    "    if initial_state is not None and cache is not None and use_cache is True:\n",
    "        raise ValueError(\"Caching and passing initial states is not possible at the same time!\")\n",
    "    \n",
    "    bsz, seq_len, _ = hidden_states.shape\n",
    "    \n",
    "    if cache is None and use_cache:\n",
    "        cache = {\n",
    "            \"conv_state\" : torch.zeros(\n",
    "                bsz, conv1d.weight.shape[0], 4\n",
    "            ),\n",
    "            \"ssm_state\" : torch.zeros(\n",
    "                bsz, num_heads, head_dim, ssm_state_size\n",
    "            ),\n",
    "            \"seq_offset\" : 0\n",
    "        }\n",
    "    cached_start = use_cache and cache[\"seq_offset\"] == 0\n",
    "    cached_forward = use_cache and cache[\"seq_offset\"] > 0\n",
    "    if cached_forward:\n",
    "        hidden_states = hidden_states.squeeze(1)\n",
    "    \n",
    "    zxbcdt = in_proj(hidden_states)\n",
    "    d_mlp = (zxbcdt.shape[-1] - 2 * intermediate_size - 2 * ssm_state_size - num_heads) // 2\n",
    "    z0, x0, z, xBC, dt = torch.split(\n",
    "        zxbcdt,\n",
    "        [d_mlp, d_mlp, intermediate_size, intermediate_size + 2 * ssm_state_size, num_heads],\n",
    "        dim=-1\n",
    "    )\n",
    "\n",
    "    if cached_start:\n",
    "        xBC_t = rearrange(xBC, \"b l d -> b d l\")\n",
    "        cache[\"conv_state\"].copy_(F.pad(xBC_t, (4 - xBC_t.shape[-1], 0)))\n",
    "\n",
    "    if cached_forward:\n",
    "        cache[\"conv_state\"].copy_(torch.roll(cache[\"conv_state\"], shifts=-1, dims=-1))\n",
    "        cache[\"conv_state\"][:, :, -1] = xBC\n",
    "        xBC = torch.sum(cache[\"conv_state\"] * rearrange(conv1d.weight, \"d 1 w -> d w\"), dim=-1)\n",
    "        if conv1d.bias is not None:\n",
    "            xBC = xBC + conv1d.bias\n",
    "        xBC = F.silu(xBC)\n",
    "    else:\n",
    "        xBC = F.silu(\n",
    "            conv1d(xBC.transpose(1, 2))[..., :seq_len].transpose(1, 2)\n",
    "        )\n",
    "\n",
    "    A = -torch.exp(A_log)\n",
    "    x, B, C = torch.split(\n",
    "        xBC, [intermediate_size, ssm_state_size, ssm_state_size], dim=-1\n",
    "    )\n",
    "    \n",
    "    init_state = initial_state if not cached_forward else cache[\"ssm_state\"] \n",
    "    x_pattern = \"b l (h p) -> b l h p\" if not cached_forward else \"b (h p) -> b 1 h p\"\n",
    "    BC_pattern = \"b l n -> b l 1 n\" if not cached_forward else \"b n -> b 1 1 n\"\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    y = mamba_chunk_scan_combined(\n",
    "        x=rearrange(x, pattern=x_pattern, p=head_dim).to(device),\n",
    "        dt=dt.to(device) if not cached_forward else dt.unsqueeze(1).to(device),\n",
    "        A=A.to(device) if not cached_forward else A.to(device=device, dtype=torch.float32),\n",
    "        B=rearrange(B, pattern=BC_pattern).to(device),\n",
    "        C=rearrange(C, pattern=BC_pattern).to(device),\n",
    "        chunk_size=chunk_size,\n",
    "        D=D.to(device) if not cached_forward else D.to(device=device, dtype=torch.float32),\n",
    "        z=None,\n",
    "        initial_states=init_state.to(device) if init_state is not None else None,\n",
    "        dt_bias=dt_bias.to(device),\n",
    "        dt_softplus=True,\n",
    "        seq_idx=None,\n",
    "        dt_min=0.0,\n",
    "        dt_max=float(\"inf\"),\n",
    "        return_final_states=return_final_state or use_cache\n",
    "    )\n",
    "    if return_final_state or use_cache:\n",
    "        y, last_state = y\n",
    "        \n",
    "    y = rearrange(y, \"b l h p -> b l (h p)\")\n",
    "    y = out_proj(y.to(\"cpu\"))\n",
    "\n",
    "    returned_last_state = None\n",
    "    if return_final_state:\n",
    "        returned_last_state = last_state\n",
    "    \n",
    "    out = (y, returned_last_state,)\n",
    "    if use_cache:\n",
    "        cache[\"ssm_state\"].copy_(last_state)\n",
    "        cache[\"seq_offset\"] = y.shape[1] if cache[\"seq_offset\"] == 0 else cache[\"seq_offset\"] + 1\n",
    "        out += (cache,)\n",
    "        \n",
    "    return out"
   ],
   "id": "f94f39079998f4be",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-12T23:16:23.374723Z",
     "start_time": "2024-06-12T23:16:02.711885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hidden_states = torch.randn(size=(2, 256, 768), dtype=torch.float32)\n",
    "\n",
    "\n",
    "# normal forward\n",
    "res_1, _ = forward(hidden_states)\n",
    "\n",
    "# cached forward\n",
    "tmp, _, cache = forward(hidden_states[:, :-1, :], use_cache=True)\n",
    "print(abs(res_1[:, :255, :] - tmp).sum())\n",
    "print(torch.allclose(tmp, res_1[:, :255, :], atol=0.01, rtol=0.01))\n",
    "\n",
    "res_2, _, _ = forward(hidden_states[:, -1, :].unsqueeze(1), use_cache=True, cache=cache)\n",
    "res_2 = torch.cat((tmp, res_2), dim=1)\n",
    "print(abs(res_1 - res_2).sum())\n",
    "print(torch.allclose(res_1, res_2, atol=0.01, rtol=0.01))"
   ],
   "id": "62464fc00e61dba5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0129, grad_fn=<SumBackward0>)\n",
      "True\n",
      "tensor(0.0157, grad_fn=<SumBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
